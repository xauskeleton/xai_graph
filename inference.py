# TÃªn file: inference_with_utils.py
# (PhiÃªn báº£n chuáº©n, sá»­ dá»¥ng model.py vÃ  utils.py)

import torch
import os
from PIL import Image
import torchvision.transforms as transforms
from transformers import AutoTokenizer

# --- 1. IMPORT Tá»ª FILE Cá»¦A Báº N ---
try:
    # Import class model
    from model import ImageCaptioningModel
    
    # Import cÃ¡c hÃ m tiá»‡n Ã­ch
    from utils import load_checkpoint, clean_sentence
    
except ImportError as e:
    print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file 'model.py' hoáº·c 'utils.py'.")
    print(f"   Chi tiáº¿t: {e}")
    print("   HÃ£y cháº¯c cháº¯n file nÃ y náº±m cÃ¹ng thÆ° má»¥c vá»›i 2 file Ä‘Ã³.")
    input("Nháº¥n Enter Ä‘á»ƒ thoÃ¡t...")
    exit()

# --- 2. CÃ€I Äáº¶T ÄÆ¯á»œNG DáºªN ---
# â¬‡ï¸ THAY Äá»”I ÄÆ¯á»œNG DáºªN áº¢NH VÃ€ CHECKPOINT Táº I ÄÃ‚Y â¬‡ï¸

IMAGE_PATH_TO_TEST = "view.jpg"  # <--- (1) Sá»¬A ÄÆ¯á»œNG DáºªN áº¢NH NÃ€Y
CHECKPOINT_PATH_TO_LOAD = "checkpoints/best_model.pth.tar" # <--- (2) Sá»¬A Náº¾U Cáº¦N

# --- ------------------------------------- ---

# Cáº¥u hÃ¬nh chung
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MAX_CAPTION_LENGTH = 50 


# --- 3. HÃ€M Há»– TRá»¢ ---

def preprocess_image(image_path):
    """ Táº£i vÃ  tiá»n xá»­ lÃ½ áº£nh Ä‘áº§u vÃ o. """
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"âŒ Lá»—i khi má»Ÿ áº£nh: {image_path}")
        print(f"   Chi tiáº¿t: {e}")
        return None
    
    # Chá»‰ cáº§n ToTensor, model sáº½ tá»± xá»­ lÃ½ pháº§n cÃ²n láº¡i
    transform = transforms.Compose([
        transforms.ToTensor()
    ])
    image_tensor = transform(image)
    return image_tensor


# --- 4. HÃ€M MAIN Äá»‚ CHáº Y INFERENCE ---
def main_inference():
    """
    HÃ m chÃ­nh: Táº£i model, xá»­ lÃ½ áº£nh vÃ  sinh caption.
    """
    
    # --- A. Kiá»ƒm tra file tá»“n táº¡i ---
    if not os.path.exists(IMAGE_PATH_TO_TEST):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y áº£nh táº¡i: {IMAGE_PATH_TO_TEST}")
        print("   Vui lÃ²ng cáº­p nháº­t biáº¿n 'IMAGE_PATH_TO_TEST'.")
        return
    if not os.path.exists(CHECKPOINT_PATH_TO_LOAD):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y checkpoint táº¡i: {CHECKPOINT_PATH_TO_LOAD}")
        print("   Vui lÃ²ng cáº­p nháº­t biáº¿n 'CHECKPOINT_PATH_TO_LOAD'.")
        return

    print(f"--- ğŸš€ Báº¯t Ä‘áº§u Script Inference ---")
    print(f"Sá»­ dá»¥ng thiáº¿t bá»‹: {DEVICE}")

    # --- B. Táº£i Checkpoint (Chá»‰ láº¥y config) ---
    print(f"--- âŒ› Äang Ä‘á»c config tá»« checkpoint: {CHECKPOINT_PATH_TO_LOAD} ---")
    try:
        checkpoint = torch.load(CHECKPOINT_PATH_TO_LOAD, map_location=DEVICE)
    except Exception as e:
        print(f"âŒ Lá»–I: KhÃ´ng thá»ƒ táº£i file checkpoint. File cÃ³ thá»ƒ bá»‹ há»ng.")
        print(f"   Chi tiáº¿t: {e}")
        return

    # --- C. Láº¤Y Cáº¤U HÃŒNH Tá»ª CHECKPOINT ---
    try:
        model_config = checkpoint['config']
        print("--- âš™ï¸ ÄÃ£ táº£i cáº¥u hÃ¬nh model tá»« checkpoint ---")
    except KeyError:
        print("âŒ Lá»–I: Checkpoint nÃ y khÃ´ng chá»©a 'config'.")
        print("   File checkpoint nÃ y cÃ³ thá»ƒ Ä‘Ã£ cÅ©.")
        print("   Vui lÃ²ng huáº¥n luyá»‡n láº¡i model vá»›i file train.py má»›i nháº¥t.")
        return

    # --- D. Táº£i Tokenizer ---
    print(f"--- ğŸ¤– Äang táº£i Tokenizer ({model_config['tokenizer']}) ---")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_config['tokenizer'])
        model_config['vocab_size'] = tokenizer.vocab_size # Cáº­p nháº­t vocab_size
    except Exception as e:
        print(f"âŒ Lá»–I: KhÃ´ng thá»ƒ táº£i Tokenizer '{model_config['tokenizer']}'.")
        print("   Vui lÃ²ng kiá»ƒm tra káº¿t ná»‘i internet.")
        return

    # --- E. Khá»Ÿi táº¡o Model ---
    print("\n--- ğŸ› ï¸ Äang khá»Ÿi táº¡o Model ---")
    model = ImageCaptioningModel(
        vocab_size=model_config['vocab_size'],
        embed_dim=model_config['embed_dim'],
        hidden_dim=model_config['hidden_dim'],
        num_objects=model_config['num_objects'],
        gnn_layers=model_config['gnn_layers'],
        gnn_heads=model_config['gnn_heads'],
        k_neighbors=model_config['k_neighbors']
    ).to(DEVICE)
    
    # --- F. Táº£i trá»ng sá»‘ (Sá»¬ Dá»¤NG HÃ€M Tá»ª UTILS.PY) ---
    print("--- ğŸ’¾ Äang táº£i trá»ng sá»‘ (state_dict) ---")
    try:
        # Sá»­ dá»¥ng hÃ m load_checkpoint tá»« utils.py
        # Giáº£ Ä‘á»‹nh hÃ m load_checkpoint cá»§a báº¡n chá»‰ load state_dict
        # vÃ  tráº£ vá» model Ä‘Ã£ load.
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()  # QUAN TRá»ŒNG: Chuyá»ƒn sang cháº¿ Ä‘á»™ dá»± Ä‘oÃ¡n
        print("âœ… Táº£i model thÃ nh cÃ´ng!")
    except Exception as e:
        print("âŒ Lá»–I: KhÃ´ng thá»ƒ táº£i state_dict. Cáº¥u hÃ¬nh model cÃ³ thá»ƒ khÃ´ng khá»›p.")
        print(f"   Chi tiáº¿t: {e}")
        return

    # --- G. Táº£i vÃ  xá»­ lÃ½ áº£nh ---
    print(f"\n--- ğŸï¸ Äang xá»­ lÃ½ áº£nh: {IMAGE_PATH_TO_TEST} ---")
    image_tensor = preprocess_image(IMAGE_PATH_TO_TEST)
    if image_tensor is None:
        return
    
    image_batch_list = [image_tensor.to(DEVICE)]
    
    # --- H. Sinh Caption ---
    print("--- âœï¸ Äang sinh caption... ---")
    
    generated_ids = None
    with torch.no_grad():
        generated_ids = model.generate_caption(
            image_batch_list, 
            max_length=MAX_CAPTION_LENGTH
        )
    
    caption_ids = generated_ids[0] 
    
    # --- I. Decode vÃ  In káº¿t quáº£ ---
    caption_str_raw = tokenizer.decode(
        caption_ids, 
        skip_special_tokens=False
    )
    
    # Sá»¬ Dá»¤NG HÃ€M Tá»ª UTILS.PY
    final_caption = clean_sentence(caption_str_raw)
    
    print("\n" + "="*50)
    print(f"áº¢nh Ä‘áº§u vÃ o: {IMAGE_PATH_TO_TEST}")
    print(f"âœï¸ Caption dá»± Ä‘oÃ¡n:")
    print(f"   {final_caption}")
    print("="*50)


# --- 5. CHáº Y TRá»°C TIáº¾P ---
if __name__ == "__main__":
    try:
        main_inference()
    except Exception as e:
        print(f"\nâŒ Lá»–I CHÆ¯Æ NG TRÃŒNH KHÃ”NG MONG MUá»N: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n--- âœ… ÄÃ£ cháº¡y xong! ---")
    try:
        input("Nháº¥n Enter Ä‘á»ƒ thoÃ¡t...")
    except EOFError:
        pass