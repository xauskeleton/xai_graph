import torch
from nltk.translate.bleu_score import corpus_bleu
import re

def count_parameters(model, trainable_only=True):
    """Äáº¿m sá»‘ parameters"""
    if trainable_only:
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    return sum(p.numel() for p in model.parameters())


def load_checkpoint(model, checkpoint_path, device='cuda'):
    """Load model checkpoint"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    return model


def save_checkpoint(state, path):
    """
    LÆ°u checkpoint (state dictionary) vÃ o má»™t file.
    Args:
        state (dict): Dictionary chá»©a má»i thá»© báº¡n muá»‘n lÆ°u.
        path (str): ÄÆ°á»ng dáº«n file Ä‘á»ƒ lÆ°u.
    """
    print("=> Äang lÆ°u checkpoint...")
    torch.save(state, path)
    print(f"=> ÄÃ£ lÆ°u checkpoint táº¡i {path}")


def get_model_summary(model):
    """In tÃ³m táº¯t model"""
    total_params = count_parameters(model, trainable_only=False)
    trainable_params = count_parameters(model, trainable_only=True)
    
    print("=" * 60)
    print("MODEL SUMMARY")
    print("=" * 60)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {total_params - trainable_params:,}")
    print("=" * 60)
    
    # Breakdown by component
    print("\nComponent breakdown:")
    for name, module in model.named_children():
        params = sum(p.numel() for p in module.parameters())
        trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
        print(f"  {name}: {params:,} ({trainable:,} trainable)")
    print("=" * 60)


# (Báº¡n cÃ³ thá»ƒ láº¥y PAD_TOKEN_ID tá»« tokenizer, nhÆ°ng dÃ¹ng string dá»… hÆ¡n)
PAD_TOKEN_STR = '<pad>'
BOS_TOKEN_STR = '<s>'
EOS_TOKEN_STR = '</s>'


def clean_sentence(sentence_str):
    """
    HÃ m helper Ä‘á»ƒ dá»n dáº¹p cÃ¢u sau khi decode tá»« tokenizer
    """
    # XÃ³a padding
    sentence_str = sentence_str.replace(PAD_TOKEN_STR, '')

    # XÃ³a token báº¯t Ä‘áº§u cÃ¢u
    sentence_str = sentence_str.replace(BOS_TOKEN_STR, '')

    # XÃ³a token káº¿t thÃºc cÃ¢u
    sentence_str = sentence_str.replace(EOS_TOKEN_STR, '')

    # XÃ³a khoáº£ng tráº¯ng thá»«a á»Ÿ Ä‘áº§u/cuá»‘i
    return sentence_str.strip()


def calculate_bleu_scores(predictions, references):
    """
    TÃ­nh Ä‘iá»ƒm BLEU-1 Ä‘áº¿n BLEU-4 sá»­ dá»¥ng NLTK.

    Args:
        predictions (list): List cÃ¡c cÃ¢u dá»± Ä‘oÃ¡n (list[str])
        references (list): List Cá»¦A CÃC list cÃ¢u tham chiáº¿u (list[list[str]])
                           Má»—i áº£nh cÃ³ thá»ƒ cÃ³ 1 hoáº·c nhiá»u cÃ¢u tham chiáº¿u.
                           Trong trÆ°á»ng há»£p cá»§a báº¡n, nÃ³ sáº½ lÃ  list[list[str]] vá»›i 1 cÃ¢u.

    Returns:
        dict: Má»™t dict chá»©a Ä‘iá»ƒm BLEU-1, BLEU-2, BLEU-3, BLEU-4.
    """

    # --- CHUáº¨N Bá»Š Dá»® LIá»†U CHO NLTK ---
    # 1. TÃ¡ch cÃ¡c cÃ¢u thÃ nh list cÃ¡c tá»« (token)
    #    VÃ­ dá»¥: "con mÃ¨o" -> ["con", "mÃ¨o"]
    pred_tokens = [sentence.split() for sentence in predictions]

    # 2. Äá»‹nh dáº¡ng láº¡i references
    #    VÃ­ dá»¥: [["con mÃ¨o"]] -> [[["con", "mÃ¨o"]]]
    ref_tokens = [[sentence.split() for sentence in ref_list] for ref_list in references]

    print(f"\n--- ğŸ“ˆ Äang tÃ­nh Ä‘iá»ƒm BLEU cho {len(pred_tokens)} máº«u ---")

    # TÃ­nh Ä‘iá»ƒm
    bleu_1 = corpus_bleu(ref_tokens, pred_tokens, weights=(1.0, 0, 0, 0))
    bleu_2 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0))
    bleu_3 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0))
    bleu_4 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25))

    scores = {
        "BLEU-1": bleu_1 * 100,
        "BLEU-2": bleu_2 * 100,
        "BLEU-3": bleu_3 * 100,
        "BLEU-4": bleu_4 * 100  # ÄÃ¢y lÃ  chá»‰ sá»‘ thÆ°á»ng Ä‘Æ°á»£c bÃ¡o cÃ¡o nháº¥t
    }

    return scores