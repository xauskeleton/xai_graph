# T√™n file: src/train.py
# (ƒê√É S·ª¨A L·ªñI `AttributeError: 'list' object has no attribute 'to'`)

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer
import os
from tqdm import tqdm # Th∆∞ vi·ªán ƒë·ªÉ xem ti·∫øn ƒë·ªô (pip install tqdm)

# Import t·ª´ c√°c file c·ªßa b·∫°n
from data_loader import get_loader
from model import ImageCaptioningModel
from utils import save_checkpoint, load_checkpoint, get_model_summary

# --- 1. C·∫§U H√åNH (CONFIG) ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}")

# ƒê∆∞·ªùng d·∫´n (gi·ªëng trong data_loader.py)
TRAIN_JSON_PATH = 'data/train_data.json' 
TRAIN_IMAGE_DIR = 'data/train/'
TEST_JSON_PATH = 'data/test_data.json'
TEST_IMAGE_DIR = 'data/test/' 

# Hyperparameters
VOCAB_SIZE = 30522 # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t t·ª´ tokenizer
EMBED_DIM = 256
HIDDEN_DIM = 512
NUM_OBJECTS = 36
GNN_LAYERS = 3
GNN_HEADS = 4
K_NEIGHBORS = 10
NUM_EPOCHS = 30
BATCH_SIZE = 16
LEARNING_RATE = 2e-5

# Checkpoint
CHECKPOINT_DIR = "checkpoints"
CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, "best_model.pth.tar")
if not os.path.exists(CHECKPOINT_DIR):
    os.makedirs(CHECKPOINT_DIR)

# --- 2. H√ÄM CH·∫†Y (MAIN) ---
def main():
    
    # --- T·∫£i Tokenizer ---
    print("--- ü§ñ ƒêang t·∫£i Tokenizer (PhoBERT) ---")
    tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')
    VOCAB_SIZE = tokenizer.vocab_size # L·∫•y vocab size ch√≠nh x√°c
    PAD_TOKEN_ID = tokenizer.pad_token_id

    # --- T·∫£i DataLoaders ---
    print("\n--- üöÄ ƒêang kh·ªüi t·∫°o DataLoaders ---")
    train_loader, _ = get_loader(
        TRAIN_JSON_PATH, TRAIN_IMAGE_DIR, tokenizer, 
        BATCH_SIZE, shuffle=True, num_workers=4
    )
    test_loader, _ = get_loader(
        TEST_JSON_PATH, TEST_IMAGE_DIR, tokenizer, 
        BATCH_SIZE, shuffle=False, num_workers=4
    )
    print("‚úÖ T·∫£i DataLoaders th√†nh c√¥ng!")

    # --- Kh·ªüi t·∫°o Model, Loss, Optimizer ---
    print("\n--- üõ†Ô∏è ƒêang kh·ªüi t·∫°o Model ---")
    model = ImageCaptioningModel(
        vocab_size=VOCAB_SIZE,
        embed_dim=EMBED_DIM,
        hidden_dim=HIDDEN_DIM,
        num_objects=NUM_OBJECTS,
        gnn_layers=GNN_LAYERS,
        gnn_heads=GNN_HEADS,
        k_neighbors=K_NEIGHBORS
    ).to(DEVICE)
    
    get_model_summary(model) # In t√≥m t·∫Øt model
    
    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=LEARNING_RATE,
        weight_decay=1e-5
    )
    
    best_test_loss = float('inf')


    # --- 3. V√íNG L·∫∂P HU·∫§N LUY·ªÜN ---
    print(f"\n--- üî• B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN V·ªöI {NUM_EPOCHS} EPOCHS ---")
    
    for epoch in range(NUM_EPOCHS):
        print(f"\n--- Epoch [{epoch+1}/{NUM_EPOCHS}] ---")
        
        # --- Giai ƒëo·∫°n TRAIN ---
        model.train()
        train_loss = 0.0

        loop = tqdm(train_loader, leave=True)
        # S·ª≠a `images` th√†nh `images_list` ƒë·ªÉ r√µ r√†ng
        for i, (images_list, token_batch) in enumerate(loop): 
            
            # --- S·ª¨A D√íNG N√ÄY ---
            # Chuy·ªÉn t·ª´ng ·∫£nh trong list sang DEVICE
            images = [img.to(DEVICE) for img in images_list]
            # --- K·∫æT TH√öC S·ª¨A ---
            
            input_ids = token_batch['input_ids'].to(DEVICE)
            
            captions_input = input_ids[:, :-1]
            captions_target = input_ids[:, 1:]
            
            # 1. Forward pass
            outputs = model(images, captions_input) # `images` gi·ªù l√† 1 list
            
            # 2. T√≠nh Loss
            loss = criterion(
                outputs.reshape(-1, VOCAB_SIZE),
                captions_target.reshape(-1)
            )
            
            # 3. Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            loop.set_description(f"Epoch [{epoch+1}/{NUM_EPOCHS}]")
            loop.set_postfix(train_loss=loss.item())
            
        avg_train_loss = train_loss / len(train_loader)

        # --- Giai ƒëo·∫°n EVALUATE (Test) ---
        model.eval()
        test_loss = 0.0
        
        with torch.no_grad():
            # S·ª≠a `images` th√†nh `images_list`
            for images_list, token_batch in test_loader:
                
                # --- S·ª¨A D√íNG N√ÄY ---
                images = [img.to(DEVICE) for img in images_list]
                # --- K·∫æT TH√öC S·ª¨A ---
                
                input_ids = token_batch['input_ids'].to(DEVICE)
                
                captions_input = input_ids[:, :-1]
                captions_target = input_ids[:, 1:]
                
                outputs = model(images, captions_input) # `images` l√† 1 list
                loss = criterion(
                    outputs.reshape(-1, VOCAB_SIZE),
                    captions_target.reshape(-1)
                )
                test_loss += loss.item()

        avg_test_loss = test_loss / len(test_loader)
        
        # --- 4. IN K·∫æT QU·∫¢ & L∆ØU MODEL ---
        print(f"--- ‚≠êÔ∏è K·∫æT TH√öC EPOCH {epoch+1} ---")
        print(f"   Loss Hu·∫•n luy·ªán (Train Loss): {avg_train_loss:.4f}")
        print(f"   Loss Ki·ªÉm tra (Test Loss):  {avg_test_loss:.4f}")
        
        if avg_test_loss < best_test_loss:
            print(f"   (Test Loss gi·∫£m t·ª´ {best_test_loss:.4f} xu·ªëng {avg_test_loss:.4f}. ƒêang l∆∞u model...)")
            best_test_loss = avg_test_loss
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_test_loss
            }
            save_checkpoint(checkpoint, CHECKPOINT_FILE)
        else:
            print(f"   (Test Loss kh√¥ng c·∫£i thi·ªán so v·ªõi {best_test_loss:.4f})")

    print("\n--- ‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! ---")

if __name__ == "__main__":
    try:
        main()
    except FileNotFoundError as e:
        print(f"\n‚ùå L·ªñI KH·ªûI T·∫†O: KH√îNG T√åM TH·∫§Y FILE.")
        print(f"   {e}")
        print("   H√£y ch·∫Øc ch·∫Øn r·∫±ng ƒë∆∞·ªùng d·∫´n trong file train.py l√† ƒê√öNG.")
    except Exception as e:
        print(f"\n‚ùå L·ªñI CH∆Ø∆†NG TR√åNH: {e}")