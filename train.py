# TÃªn file: src/train.py
# (PHIÃŠN Báº¢N NÃ‚NG Cáº¤P: ThÃªm Early Stopping, LÆ°u Config Ä‘áº§y Ä‘á»§ vÃ  Training History)

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer
import os
import json
from datetime import datetime
from tqdm import tqdm

# Import tá»« cÃ¡c file cá»§a báº¡n
from data_loader import get_loader
from model import ImageCaptioningModel
from utils import save_checkpoint, load_checkpoint, get_model_summary, calculate_bleu_scores, clean_sentence

# --- 1. Cáº¤U HÃŒNH (CONFIG) ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Äang sá»­ dá»¥ng thiáº¿t bá»‹: {DEVICE}")

# ÄÆ°á»ng dáº«n
TRAIN_JSON_PATH = 'data/train_data.json'
TRAIN_IMAGE_DIR = 'data/train/'
TEST_JSON_PATH = 'data/test_data.json'
TEST_IMAGE_DIR = 'data/test/'

# Hyperparameters
VOCAB_SIZE = 30522
EMBED_DIM = 256
HIDDEN_DIM = 1024
NUM_OBJECTS = 48
GNN_LAYERS = 4
GNN_HEADS = 4
K_NEIGHBORS = 15
NUM_EPOCHS = 30
BATCH_SIZE =64
LEARNING_RATE = 5e-5

# ğŸ’¥ Early Stopping Configuration
EARLY_STOPPING_PATIENCE = 5  # Dá»«ng sau 5 epochs khÃ´ng cáº£i thiá»‡n
MIN_DELTA = 1e-4  # NgÆ°á»¡ng cáº£i thiá»‡n tá»‘i thiá»ƒu

# Checkpoint
CHECKPOINT_DIR = "checkpoints"
LOG_DIR = "logs"  # ğŸ’¥ THÃŠM Má»šI: ThÆ° má»¥c lÆ°u logs
CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, "best_model.pth.tar")

# Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
if not os.path.exists(CHECKPOINT_DIR):
    os.makedirs(CHECKPOINT_DIR)
if not os.path.exists(LOG_DIR):  # ğŸ’¥ THÃŠM Má»šI
    os.makedirs(LOG_DIR)


# --- 2. HÃ€M CHáº Y (MAIN) ---
def main():
    # --- Táº£i Tokenizer ---
    print("--- ğŸ¤– Äang táº£i Tokenizer (PhoBERT) ---")
    tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')
    VOCAB_SIZE = tokenizer.vocab_size  # Láº¥y vocab size chÃ­nh xÃ¡c
    PAD_TOKEN_ID = tokenizer.pad_token_id

    # ğŸ’¥ THÃŠM Má»šI: Táº¡o dict cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§ vá»›i timestamp
    model_config = {
        'vocab_size': VOCAB_SIZE,
        'embed_dim': EMBED_DIM,
        'hidden_dim': HIDDEN_DIM,
        'num_objects': NUM_OBJECTS,
        'gnn_layers': GNN_LAYERS,
        'gnn_heads': GNN_HEADS,
        'k_neighbors': K_NEIGHBORS,
        'batch_size': BATCH_SIZE,
        'learning_rate': LEARNING_RATE,
        'num_epochs': NUM_EPOCHS,
        'early_stopping_patience': EARLY_STOPPING_PATIENCE,
        'min_delta': MIN_DELTA,
        'tokenizer': 'vinai/phobert-base',
        'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'device': str(DEVICE)
    }

    print(f"--- âš™ï¸ Äang huáº¥n luyá»‡n vá»›i cáº¥u hÃ¬nh: ---")
    for key, value in model_config.items():
        print(f"   {key}: {value}")

    # --- Táº£i DataLoaders ---
    print("\n--- ğŸš€ Äang khá»Ÿi táº¡o DataLoaders ---")
    train_loader, _ = get_loader(
        TRAIN_JSON_PATH, TRAIN_IMAGE_DIR, tokenizer,
        BATCH_SIZE, shuffle=True, num_workers=4
    )
    test_loader, _ = get_loader(
        TEST_JSON_PATH, TEST_IMAGE_DIR, tokenizer,
        BATCH_SIZE, shuffle=False, num_workers=4
    )
    print("âœ… Táº£i DataLoaders thÃ nh cÃ´ng!")

    # --- Khá»Ÿi táº¡o Model, Loss, Optimizer ---
    print("\n--- ğŸ› ï¸ Äang khá»Ÿi táº¡o Model ---")
    model = ImageCaptioningModel(
        vocab_size=model_config['vocab_size'],
        embed_dim=model_config['embed_dim'],
        hidden_dim=model_config['hidden_dim'],
        num_objects=model_config['num_objects'],
        gnn_layers=model_config['gnn_layers'],
        gnn_heads=model_config['gnn_heads'],
        k_neighbors=model_config['k_neighbors']
    ).to(DEVICE)

    get_model_summary(model)

    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=LEARNING_RATE,
        weight_decay=1e-5
    )

    best_test_loss = float('inf')
    best_bleu_4 = 0.0  # ğŸ’¥ THÃŠM: Track best BLEU-4

    # ğŸ’¥ THÃŠM Má»šI: Early Stopping variables
    epochs_no_improve = 0
    best_epoch = 0

    # ğŸ’¥ THÃŠM Má»šI: Training history Ä‘á»ƒ lÆ°u vÃ o log
    training_history = {
        'config': model_config,
        'epochs': [],
        'best_epoch': 0,
        'best_test_loss': float('inf'),
        'best_bleu_4': 0.0
    }

    # --- 3. VÃ’NG Láº¶P HUáº¤N LUYá»†N ---
    print(f"\n--- ğŸ”¥ Báº®T Äáº¦U HUáº¤N LUYá»†N Vá»šI {NUM_EPOCHS} EPOCHS ---")

    for epoch in range(NUM_EPOCHS):
        print(f"\n{'=' * 80}")
        print(f"--- Epoch [{epoch + 1}/{NUM_EPOCHS}] ---")
        print(f"{'=' * 80}")

        # --- Giai Ä‘oáº¡n TRAIN ---
        model.train()
        train_loss = 0.0
        loop = tqdm(train_loader, leave=True)
        for i, (images_list, token_batch) in enumerate(loop):
            images = [img.to(DEVICE) for img in images_list]
            input_ids = token_batch['input_ids'].to(DEVICE)
            captions_input = input_ids[:, :-1]
            captions_target = input_ids[:, 1:]

            outputs = model(images, captions_input)
            loss = criterion(outputs.reshape(-1, VOCAB_SIZE), captions_target.reshape(-1))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            loop.set_description(f"Epoch [{epoch + 1}/{NUM_EPOCHS}]")
            loop.set_postfix(train_loss=loss.item())

        avg_train_loss = train_loss / len(train_loader)

        # --- Giai Ä‘oáº¡n EVALUATE (Test) ---
        model.eval()
        test_loss = 0.0

        all_predictions = []
        all_references = []

        print("\n--- ğŸ§ Äang cháº¡y ÄÃ¡nh giÃ¡ (Evaluate) trÃªn táº­p Test ---")

        loop_test = tqdm(test_loader, leave=True, desc="Testing")

        with torch.no_grad():
            for images_list, token_batch in loop_test:
                images = [img.to(DEVICE) for img in images_list]
                input_ids = token_batch['input_ids'].to(DEVICE)

                captions_input = input_ids[:, :-1]
                captions_target = input_ids[:, 1:]

                # TÃ­nh test loss
                outputs_logits = model(images, captions_input)
                loss = criterion(
                    outputs_logits.reshape(-1, VOCAB_SIZE),
                    captions_target.reshape(-1)
                )
                test_loss += loss.item()

                # Sinh caption Ä‘á»ƒ tÃ­nh BLEU
                generated_ids = model.generate_caption(images, max_length=50)

                # Decode
                pred_sentences_batch = tokenizer.batch_decode(
                    generated_ids,
                    skip_special_tokens=False
                )
                ref_sentences_batch = tokenizer.batch_decode(
                    input_ids,
                    skip_special_tokens=False
                )

                # Dá»n dáº¹p vÃ  lÆ°u
                for pred_str, ref_str in zip(pred_sentences_batch, ref_sentences_batch):
                    cleaned_pred = clean_sentence(pred_str)
                    cleaned_ref = clean_sentence(ref_str)
                    all_predictions.append(cleaned_pred)
                    all_references.append([cleaned_ref])

        avg_test_loss = test_loss / len(test_loader)

        # TÃ­nh BLEU scores
        bleu_scores = calculate_bleu_scores(all_predictions, all_references)

        # --- IN Káº¾T QUáº¢ ---
        print(f"\n{'=' * 80}")
        print(f"--- â­ï¸ Káº¾T THÃšC EPOCH {epoch + 1} ---")
        print(f"{'=' * 80}")
        print(f"   ğŸ“ˆ Loss Huáº¥n luyá»‡n (Train Loss): {avg_train_loss:.4f}")
        print(f"   ğŸ“‰ Loss Kiá»ƒm tra (Test Loss):    {avg_test_loss:.4f}")
        print("   --- ğŸ“Š ÄIá»‚M BLEU (trÃªn táº­p Test) ---")
        print(f"       BLEU-1: {bleu_scores['BLEU-1']:.2f}")
        print(f"       BLEU-2: {bleu_scores['BLEU-2']:.2f}")
        print(f"       BLEU-3: {bleu_scores['BLEU-3']:.2f}")
        print(f"       BLEU-4: {bleu_scores['BLEU-4']:.2f}  â­ (Chá»‰ sá»‘ quan trá»ng)")

        # ğŸ’¥ THÃŠM: LÆ°u epoch info vÃ o history
        epoch_info = {
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'test_loss': avg_test_loss,
            'bleu_1': bleu_scores['BLEU-1'],
            'bleu_2': bleu_scores['BLEU-2'],
            'bleu_3': bleu_scores['BLEU-3'],
            'bleu_4': bleu_scores['BLEU-4']
        }
        training_history['epochs'].append(epoch_info)

        # --- LOGIC EARLY STOPPING ---
        # Kiá»ƒm tra xem loss cÃ³ cáº£i thiá»‡n khÃ´ng
        if avg_test_loss < (best_test_loss - MIN_DELTA):
            # Loss cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
            print(f"\n   âœ… Test Loss cáº£i thiá»‡n: {best_test_loss:.4f} â†’ {avg_test_loss:.4f}")
            print(f"   ğŸ’¾ Äang lÆ°u model tá»‘t nháº¥t...")

            best_test_loss = avg_test_loss
            best_bleu_4 = bleu_scores['BLEU-4']
            best_epoch = epoch + 1
            epochs_no_improve = 0  # Reset counter

            # ğŸ’¥ THÃŠM: LÆ°u checkpoint vá»›i config Ä‘áº§y Ä‘á»§
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'test_loss': avg_test_loss,
                'bleu_scores': bleu_scores,
                'config': model_config,  # ğŸ’¥ LÆ°u config
                'training_history': training_history  # ğŸ’¥ LÆ°u history
            }
            save_checkpoint(checkpoint, CHECKPOINT_FILE)

            # ğŸ’¥ Update training history
            training_history['best_epoch'] = best_epoch
            training_history['best_test_loss'] = best_test_loss
            training_history['best_bleu_4'] = best_bleu_4

        else:
            # Loss khÃ´ng cáº£i thiá»‡n
            epochs_no_improve += 1
            print(f"\n   âš ï¸ Test Loss khÃ´ng cáº£i thiá»‡n (Best: {best_test_loss:.4f})")
            print(f"   ğŸ“Š Early Stopping Counter: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}")

            # ğŸ’¥ KIá»‚M TRA EARLY STOPPING
            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:
                print(f"\n{'=' * 80}")
                print(f"ğŸ›‘ EARLY STOPPING Ä‘Æ°á»£c kÃ­ch hoáº¡t táº¡i epoch {epoch + 1}!")
                print(f"{'=' * 80}")
                print(f"   ğŸ“Œ Best Epoch: {best_epoch}")
                print(f"   ğŸ“‰ Best Test Loss: {best_test_loss:.4f}")
                print(f"   ğŸ“Š Best BLEU-4: {best_bleu_4:.2f}")
                print(f"   â±ï¸ ÄÃ£ khÃ´ng cáº£i thiá»‡n sau {EARLY_STOPPING_PATIENCE} epochs")
                print(f"{'=' * 80}")
                break  # ğŸ’¥ Dá»ªNG TRAINING

        # ğŸ’¥ THÃŠM: LÆ°u training log má»—i 5 epochs
        if (epoch + 1) % 5 == 0:
            log_file = os.path.join(LOG_DIR, f"training_log_epoch_{epoch + 1}.json")
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(training_history, f, indent=4, ensure_ascii=False)
            print(f"   ğŸ“ ÄÃ£ lÆ°u training log: {log_file}")

    # --- Káº¾T THÃšC TRAINING ---
    print(f"\n{'=' * 80}")
    print("--- âœ… Huáº¥n luyá»‡n hoÃ n táº¥t! ---")
    print(f"{'=' * 80}")
    print(f"   ğŸ“Š Tá»•ng sá»‘ epochs Ä‘Ã£ cháº¡y: {len(training_history['epochs'])}")
    print(f"   ğŸ† Best Epoch: {training_history['best_epoch']}")
    print(f"   ğŸ“‰ Best Test Loss: {training_history['best_test_loss']:.4f}")
    print(f"   ğŸ“Š Best BLEU-4: {training_history['best_bleu_4']:.2f}")

    # ğŸ’¥ THÃŠM: LÆ°u final training log
    final_log_file = os.path.join(LOG_DIR, "training_log_final.json")
    with open(final_log_file, 'w', encoding='utf-8') as f:
        json.dump(training_history, f, indent=4, ensure_ascii=False)
    print(f"\n   ğŸ“ Files Ä‘Ã£ lÆ°u:")
    print(f"      âœ… Model checkpoint: {CHECKPOINT_FILE}")
    print(f"      âœ… Training log: {final_log_file}")
    print(f"{'=' * 80}\n")


if __name__ == "__main__":
    try:
        main()
    except FileNotFoundError as e:
        print(f"\nâŒ Lá»–I KHá»I Táº O: KHÃ”NG TÃŒM THáº¤Y FILE.")
        print(f"   {e}")
        print("   HÃ£y cháº¯c cháº¯n ráº±ng Ä‘Æ°á»ng dáº«n trong file train.py lÃ  ÄÃšNG.")
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Training bá»‹ giÃ¡n Ä‘oáº¡n bá»Ÿi ngÆ°á»i dÃ¹ng!")
    except Exception as e:
        print(f"\nâŒ Lá»–I TRONG QUÃ TRÃŒNH CHáº Y: {e}")
        import traceback

        traceback.print_exc()