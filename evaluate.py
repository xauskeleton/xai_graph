# TÃªn file: src/evaluate.py
# (PhiÃªn báº£n cháº¡y trá»±c tiáº¿p, khÃ´ng cáº§n terminal)

import torch
import torch.nn as nn
from transformers import AutoTokenizer
import os
import json
from tqdm import tqdm
# import argparse # <-- KHÃ”NG Cáº¦N Ná»®A

# Import tá»« cÃ¡c file cá»§a báº¡n
from data_loader import get_loader
from model import ImageCaptioningModel
from utils import load_checkpoint, clean_sentence, get_model_summary

# Import thÆ° viá»‡n metrics
import aac_metrics
os.environ['JAVA_TOOL_OPTIONS'] = '-Xmx4096M'

# --- 1. Cáº¤U HÃŒNH (PHáº¢I GIá»NG Há»†T train.py) ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Äang sá»­ dá»¥ng thiáº¿t bá»‹: {DEVICE}")

# ÄÆ°á»ng dáº«n (Chá»‰ cáº§n táº­p TEST)
TEST_JSON_PATH = 'data/test_data.json' 
TEST_IMAGE_DIR = 'data/test/' 

# Hyperparameters (SAO CHÃ‰P Tá»ª FILE train.py Cá»¦A Báº N)
VOCAB_SIZE = 30522 
EMBED_DIM = 256
HIDDEN_DIM = 1024       
NUM_OBJECTS = 48        
GNN_LAYERS = 4          
GNN_HEADS = 4
K_NEIGHBORS = 15   
BATCH_SIZE = 2        

# --- 2. HÃ€M CHáº Y (MAIN) ---
def evaluate():
    
    # --- ğŸ’¥ THAY Äá»”I á» ÄÃ‚Y ğŸ’¥ ---
    # Tá»± Ä‘iá»n Ä‘Æ°á»ng dáº«n Ä‘áº¿n checkpoint cá»§a báº¡n
    CHECKPOINT_FILE = "checkpoints/best_model.pth.tar" # <--- Sá»¬A ÄÆ¯á»œNG DáºªN NÃ€Y
    
    # --- Káº¾T THÃšC THAY Äá»”I ---

    if not os.path.exists(CHECKPOINT_FILE):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y checkpoint táº¡i: {CHECKPOINT_FILE}")
        return

    # --- 1. Táº£i dependencies cho Java (SPICE/METEOR) ---
    try:
        print("\n--- ğŸ“¦ Äang kiá»ƒm tra/táº£i dependencies cho aac-metrics (Java)... ---")
        aac_metrics.download_java_dependencies(force=False)
        print("âœ… Dependencies OK!")
    except Exception as e:
        print(f"âš ï¸ Lá»–I khi táº£i dependencies cho Java: {e}")
        print("   -> Metric SPICE vÃ  METEOR cÃ³ thá»ƒ sáº½ khÃ´ng hoáº¡t Ä‘á»™ng.")
        print("   -> (Náº¿u á»Ÿ trÃªn server, hÃ£y thá»­ cháº¡y: sudo apt install default-jdk)")

    # --- 2. Táº£i Tokenizer ---
    print("--- ğŸ¤– Äang táº£i Tokenizer (PhoBERT) ---")
    tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')
    VOCAB_SIZE = tokenizer.vocab_size 

    # --- 3. Táº£i DataLoaders (Chá»‰ cáº§n TEST) ---
    print("\n--- ğŸš€ Äang khá»Ÿi táº¡o Test DataLoader ---")
    test_loader, _ = get_loader(
        TEST_JSON_PATH, TEST_IMAGE_DIR, tokenizer, 
        BATCH_SIZE, shuffle=False, num_workers=4
    )
    print("âœ… Táº£i Test Loader thÃ nh cÃ´ng!")

    # --- 4. Khá»Ÿi táº¡o Model (Vá»šI Cáº¤U HÃŒNH ÄÃšNG) ---
    print("\n--- ğŸ› ï¸ Äang khá»Ÿi táº¡o Model ---")
    model = ImageCaptioningModel(
        vocab_size=VOCAB_SIZE,
        embed_dim=EMBED_DIM,
        hidden_dim=HIDDEN_DIM,       
        num_objects=NUM_OBJECTS,     
        gnn_layers=GNN_LAYERS,       
        gnn_heads=GNN_HEADS,
        k_neighbors=K_NEIGHBORS      
    ).to(DEVICE)
    
    # --- 5. Táº£i Checkpoint ---
    print(f"\n--- ğŸ’¾ Äang táº£i checkpoint tá»«: {CHECKPOINT_FILE} ---")
    try:
        model = load_checkpoint(model, CHECKPOINT_FILE, device=DEVICE)
        print("âœ… Táº£i checkpoint thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ Lá»–I KHI Táº¢I CHECKPOINT: {e}")
        print("   Lá»—i nÃ y thÆ°á»ng xáº£y ra khi cáº¥u hÃ¬nh model (evaluate.py) khÃ´ng khá»›p vá»›i model Ä‘Ã£ lÆ°u (train.py).")
        return

    # --- 6. Cháº¡y ÄÃ¡nh giÃ¡ ---
    model.eval() # Báº­t cháº¿ Ä‘á»™ EVAL
    all_predictions = []
    all_references = []

    print("\n--- ğŸ§ Äang cháº¡y ÄÃ¡nh giÃ¡ trÃªn toÃ n bá»™ táº­p Test ---")
    loop_test = tqdm(test_loader, leave=True, desc="Testing")

    with torch.no_grad(): # KhÃ´ng cáº§n tÃ­nh gradient
        for images_list, token_batch in loop_test:
            images = [img.to(DEVICE) for img in images_list]
            input_ids = token_batch['input_ids'].to(DEVICE)

            generated_ids = model.generate_caption(images, max_length=50)

            pred_sentences_batch = tokenizer.batch_decode(
                generated_ids,
                skip_special_tokens=False
            )
            ref_sentences_batch = tokenizer.batch_decode(
                input_ids,
                skip_special_tokens=False
            )

            for pred_str, ref_str in zip(pred_sentences_batch, ref_sentences_batch):
                all_predictions.append(clean_sentence(pred_str))
                all_references.append([clean_sentence(ref_str)])

    print("âœ… ÄÃ£ cháº¡y xong! Báº¯t Ä‘áº§u tÃ­nh Ä‘iá»ƒm...")

    # --- 7. TÃNH TOÃN VÃ€ IN ÄIá»‚M Sá» ---
    print("\n--- ğŸ“Š Äang tÃ­nh Ä‘iá»ƒm (BLEU, METEOR, ROUGE, CIDEr-D, SPICE)... ---")
    
    try:
        all_scores = aac_metrics.evaluate(all_predictions, all_references)
        
        # --- 8. IN Káº¾T QUáº¢ ---
        print("\n" + "="*50)
        print(f"Káº¾T QUáº¢ ÄÃNH GIÃ CHO CHECKPOINT: {CHECKPOINT_FILE}")
        print("="*50)
        for metric, score in all_scores.items():
            print(f"   {metric:<10}: {score:.2f}")
        print("="*50)

    except Exception as e:
        print(f"\nâŒ Lá»–I KHI TÃNH TOÃN METRICS (aac-metrics): {e}")
        print("   Lá»—i nÃ y thÆ°á»ng xáº£y ra náº¿u Java (JDK) chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t Ä‘Ãºng cÃ¡ch.")
        print("   HÃ£y thá»­ cháº¡y: sudo apt install default-jdk")


if __name__ == "__main__":
    try:
        evaluate()
    except Exception as e:
        print(f"\nâŒ Lá»–I CHÆ¯A Xá»¬ LÃ: {e}")